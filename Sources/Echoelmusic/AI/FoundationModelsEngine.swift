import Foundation
import Combine

#if canImport(FoundationModels)
import FoundationModels
#endif

// MARK: - Foundation Models Engine
// WWDC 2025: On-Device AI using Apple's Foundation Models framework
// Privacy-first, offline-capable AI for music production

/// Main engine for on-device AI capabilities using Apple's Foundation Models framework
@MainActor
@Observable
class FoundationModelsEngine {

    // MARK: - State

    /// Whether the engine is ready for inference
    var isReady: Bool = false

    /// Current processing status
    var processingStatus: ProcessingStatus = .idle

    /// Last generated result
    var lastResult: GenerationResult?

    /// Error message if any
    var errorMessage: String?

    enum ProcessingStatus: String {
        case idle = "Ready"
        case loading = "Loading model..."
        case processing = "Processing..."
        case streaming = "Generating..."
        case complete = "Complete"
        case error = "Error"
    }

    // MARK: - Generable Types for Music Production

    /// Music composition suggestion generated by on-device AI
    @Generable
    struct CompositionSuggestion: Codable, Sendable {
        @Guide(description: "Suggested tempo in BPM (60-200)")
        var tempo: Int

        @Guide(description: "Musical key (e.g., C major, A minor)")
        var key: String

        @Guide(description: "Time signature (e.g., 4/4, 3/4, 6/8)")
        var timeSignature: String

        @Guide(description: "Genre classification")
        var genre: String

        @Guide(description: "Mood descriptor")
        var mood: String

        @Guide(description: "Suggested chord progression as array")
        var chordProgression: [String]

        @Guide(description: "Brief description of the composition idea")
        var description: String
    }

    /// Lyrics generation result
    @Generable
    struct LyricsGeneration: Codable, Sendable {
        @Guide(description: "Generated verse lyrics")
        var verse: String

        @Guide(description: "Generated chorus lyrics")
        var chorus: String

        @Guide(description: "Optional bridge section")
        var bridge: String?

        @Guide(description: "Suggested rhyme scheme")
        var rhymeScheme: String

        @Guide(description: "Emotional theme of lyrics")
        var theme: String
    }

    /// Audio analysis result
    @Generable
    struct AudioAnalysis: Codable, Sendable {
        @Guide(description: "Detected genre")
        var genre: String

        @Guide(description: "Estimated tempo in BPM")
        var estimatedTempo: Int

        @Guide(description: "Detected key")
        var detectedKey: String

        @Guide(description: "Energy level 0-100")
        var energyLevel: Int

        @Guide(description: "Valence (happiness) 0-100")
        var valence: Int

        @Guide(description: "Danceability score 0-100")
        var danceability: Int

        @Guide(description: "Production suggestions")
        var suggestions: [String]
    }

    /// Mixing advice from AI
    @Generable
    struct MixingAdvice: Codable, Sendable {
        @Guide(description: "EQ recommendations")
        var eqRecommendations: [String]

        @Guide(description: "Compression settings advice")
        var compressionAdvice: String

        @Guide(description: "Stereo imaging suggestions")
        var stereoImaging: String

        @Guide(description: "Reverb and space recommendations")
        var reverbAdvice: String

        @Guide(description: "Overall mix balance feedback")
        var balanceFeedback: String

        @Guide(description: "Priority issues to fix")
        var priorityIssues: [String]
    }

    /// Bio-reactive music suggestion
    @Generable
    struct BioReactiveSuggestion: Codable, Sendable {
        @Guide(description: "Recommended tempo based on heart rate")
        var suggestedTempo: Int

        @Guide(description: "Recommended musical key for mood")
        var suggestedKey: String

        @Guide(description: "Intensity level 0-100")
        var intensity: Int

        @Guide(description: "Suggested instrument layers")
        var instruments: [String]

        @Guide(description: "Breathing pattern suggestion")
        var breathingPattern: String

        @Guide(description: "Meditation cue timing in seconds")
        var meditationCues: [Int]

        @Guide(description: "Overall wellness recommendation")
        var wellnessAdvice: String
    }

    // MARK: - Generation Result

    struct GenerationResult {
        let type: ResultType
        let content: Any
        let timestamp: Date
        let processingTime: TimeInterval

        enum ResultType {
            case composition
            case lyrics
            case analysis
            case mixing
            case bioReactive
        }
    }

    // MARK: - Session Management

    #if canImport(FoundationModels)
    private var session: LanguageModelSession?
    #endif

    private var cancellables = Set<AnyCancellable>()

    // MARK: - Initialization

    init() {
        Task {
            await initialize()
        }
    }

    private func initialize() async {
        processingStatus = .loading

        #if canImport(FoundationModels)
        do {
            // Check model availability
            let availability = LanguageModelSession.Availability.current
            guard availability == .available else {
                errorMessage = "On-device AI model not available"
                processingStatus = .error
                return
            }

            // Create session with Echoelmusic-specific configuration
            session = LanguageModelSession(
                instructions: """
                You are an expert music production AI assistant for Echoelmusic.
                You specialize in:
                - Music composition and arrangement
                - Audio mixing and mastering advice
                - Lyrics writing and song structure
                - Bio-reactive music therapy
                - Sound design and synthesis

                Provide professional, actionable advice tailored for music creators.
                Consider wellness and therapeutic applications when relevant.
                """
            )

            isReady = true
            processingStatus = .idle
            print("ðŸ§  Foundation Models Engine initialized")
        } catch {
            errorMessage = error.localizedDescription
            processingStatus = .error
            print("âŒ Foundation Models initialization failed: \(error)")
        }
        #else
        // Fallback for platforms without Foundation Models
        isReady = true
        processingStatus = .idle
        print("ðŸ§  Foundation Models Engine (compatibility mode)")
        #endif
    }

    // MARK: - Composition Generation

    /// Generate composition suggestions based on user input
    func generateComposition(
        mood: String,
        genre: String,
        duration: Int
    ) async throws -> CompositionSuggestion {
        processingStatus = .processing
        let startTime = Date()

        #if canImport(FoundationModels)
        guard let session = session else {
            throw FoundationModelsError.sessionNotInitialized
        }

        let prompt = """
        Generate a music composition suggestion for:
        - Mood: \(mood)
        - Genre: \(genre)
        - Target duration: \(duration) seconds

        Provide tempo, key, time signature, chord progression, and a brief description.
        """

        let response = try await session.respond(
            to: prompt,
            generating: CompositionSuggestion.self
        )

        let result = response.content
        let processingTime = Date().timeIntervalSince(startTime)

        lastResult = GenerationResult(
            type: .composition,
            content: result,
            timestamp: Date(),
            processingTime: processingTime
        )

        processingStatus = .complete
        return result
        #else
        // Fallback generation
        let result = CompositionSuggestion(
            tempo: genre.contains("ambient") ? 70 : 120,
            key: "C major",
            timeSignature: "4/4",
            genre: genre,
            mood: mood,
            chordProgression: ["C", "Am", "F", "G"],
            description: "A \(mood) \(genre) composition"
        )
        processingStatus = .complete
        return result
        #endif
    }

    // MARK: - Lyrics Generation

    /// Generate lyrics based on theme and style
    func generateLyrics(
        theme: String,
        style: String,
        structure: String = "verse-chorus-verse-chorus-bridge-chorus"
    ) async throws -> LyricsGeneration {
        processingStatus = .processing
        let startTime = Date()

        #if canImport(FoundationModels)
        guard let session = session else {
            throw FoundationModelsError.sessionNotInitialized
        }

        let prompt = """
        Generate song lyrics for:
        - Theme: \(theme)
        - Style: \(style)
        - Structure: \(structure)

        Include verse, chorus, and optionally a bridge. Specify the rhyme scheme.
        """

        let response = try await session.respond(
            to: prompt,
            generating: LyricsGeneration.self
        )

        let result = response.content
        processingStatus = .complete
        return result
        #else
        let result = LyricsGeneration(
            verse: "Generated verse about \(theme)",
            chorus: "Generated chorus in \(style) style",
            bridge: "Bridge section",
            rhymeScheme: "ABAB",
            theme: theme
        )
        processingStatus = .complete
        return result
        #endif
    }

    // MARK: - Audio Analysis

    /// Analyze audio characteristics and provide suggestions
    func analyzeAudio(
        frequencySpectrum: [Float],
        rmsLevel: Float,
        zeroCrossingRate: Float
    ) async throws -> AudioAnalysis {
        processingStatus = .processing

        #if canImport(FoundationModels)
        guard let session = session else {
            throw FoundationModelsError.sessionNotInitialized
        }

        // Convert spectrum to descriptive string
        let spectrumDescription = describeSpectrum(frequencySpectrum)

        let prompt = """
        Analyze this audio based on:
        - Frequency spectrum: \(spectrumDescription)
        - RMS level: \(rmsLevel)
        - Zero crossing rate: \(zeroCrossingRate)

        Provide genre detection, tempo estimation, key detection, and production suggestions.
        """

        let response = try await session.respond(
            to: prompt,
            generating: AudioAnalysis.self
        )

        processingStatus = .complete
        return response.content
        #else
        let result = AudioAnalysis(
            genre: "Electronic",
            estimatedTempo: 128,
            detectedKey: "A minor",
            energyLevel: Int(rmsLevel * 100),
            valence: 65,
            danceability: 75,
            suggestions: ["Consider adding more low-end", "Nice stereo width"]
        )
        processingStatus = .complete
        return result
        #endif
    }

    // MARK: - Mixing Advice

    /// Get AI-powered mixing advice
    func getMixingAdvice(
        trackDescription: String,
        currentIssues: [String]
    ) async throws -> MixingAdvice {
        processingStatus = .processing

        #if canImport(FoundationModels)
        guard let session = session else {
            throw FoundationModelsError.sessionNotInitialized
        }

        let prompt = """
        Provide professional mixing advice for:
        Track: \(trackDescription)
        Current issues reported: \(currentIssues.joined(separator: ", "))

        Give specific EQ, compression, stereo imaging, and reverb recommendations.
        """

        let response = try await session.respond(
            to: prompt,
            generating: MixingAdvice.self
        )

        processingStatus = .complete
        return response.content
        #else
        let result = MixingAdvice(
            eqRecommendations: ["Cut muddy frequencies around 300Hz", "Boost presence at 3kHz"],
            compressionAdvice: "Use gentle compression with 4:1 ratio",
            stereoImaging: "Pan guitars slightly left and right",
            reverbAdvice: "Use a medium room reverb with 1.5s decay",
            balanceFeedback: "Overall mix is well balanced",
            priorityIssues: currentIssues.isEmpty ? ["No major issues detected"] : currentIssues
        )
        processingStatus = .complete
        return result
        #endif
    }

    // MARK: - Bio-Reactive Suggestions

    /// Generate music suggestions based on biometric data
    func generateBioReactiveSuggestion(
        heartRate: Double,
        hrv: Double,
        coherence: Float,
        mood: String
    ) async throws -> BioReactiveSuggestion {
        processingStatus = .processing

        #if canImport(FoundationModels)
        guard let session = session else {
            throw FoundationModelsError.sessionNotInitialized
        }

        let prompt = """
        Generate a bio-reactive music recommendation based on:
        - Heart rate: \(heartRate) BPM
        - Heart rate variability: \(hrv) ms
        - Coherence score: \(coherence * 100)%
        - Current mood: \(mood)

        Suggest tempo, key, intensity, instruments, and wellness advice for optimal bio-musical alignment.
        """

        let response = try await session.respond(
            to: prompt,
            generating: BioReactiveSuggestion.self
        )

        processingStatus = .complete
        return response.content
        #else
        // Intelligent fallback based on biometrics
        let suggestedTempo = calculateOptimalTempo(heartRate: heartRate, coherence: coherence)
        let intensity = Int(coherence * 100)

        let result = BioReactiveSuggestion(
            suggestedTempo: suggestedTempo,
            suggestedKey: coherence > 0.7 ? "C major" : "A minor",
            intensity: intensity,
            instruments: coherence > 0.7
                ? ["Piano", "Strings", "Soft Pads"]
                : ["Ambient Synths", "Rain Sounds", "Soft Bass"],
            breathingPattern: "4-7-8 breathing (inhale 4s, hold 7s, exhale 8s)",
            meditationCues: [30, 60, 120, 180],
            wellnessAdvice: coherence > 0.7
                ? "Your coherence is high. Maintain this state with gentle, flowing music."
                : "Consider slowing down. Use ambient textures to increase coherence."
        )
        processingStatus = .complete
        return result
        #endif
    }

    // MARK: - Streaming Generation

    /// Stream composition suggestions with real-time updates
    func streamComposition(
        mood: String,
        genre: String,
        onUpdate: @escaping (String) -> Void
    ) async throws {
        processingStatus = .streaming

        #if canImport(FoundationModels)
        guard let session = session else {
            throw FoundationModelsError.sessionNotInitialized
        }

        let prompt = """
        Generate a detailed music composition plan for a \(mood) \(genre) track.
        Include structure, instrumentation, and arrangement ideas.
        """

        let stream = try session.streamResponse(to: prompt)

        for try await partial in stream {
            onUpdate(partial.content)
        }

        processingStatus = .complete
        #else
        // Fallback streaming simulation
        let parts = [
            "Analyzing mood: \(mood)...",
            "Selecting genre elements for \(genre)...",
            "Generating chord progression...",
            "Suggesting instrumentation...",
            "Creating arrangement structure...",
            "Complete!"
        ]

        for part in parts {
            try await Task.sleep(nanoseconds: 500_000_000)
            onUpdate(part)
        }
        processingStatus = .complete
        #endif
    }

    // MARK: - Tool Calling

    /// Register tools for AI to call during generation
    func registerMusicTools() {
        #if canImport(FoundationModels)
        // Define tools the AI can call
        let tools: [Tool] = [
            Tool(
                name: "getProjectTempo",
                description: "Get the current project tempo",
                handler: { _ in
                    return ["tempo": 120]
                }
            ),
            Tool(
                name: "setProjectKey",
                description: "Set the musical key of the project",
                parameters: [
                    .string(name: "key", description: "Musical key (e.g., C major)")
                ],
                handler: { params in
                    let key = params["key"] as? String ?? "C major"
                    return ["success": true, "key": key]
                }
            ),
            Tool(
                name: "addTrack",
                description: "Add a new track to the project",
                parameters: [
                    .string(name: "name", description: "Track name"),
                    .string(name: "type", description: "Track type (audio, midi, aux)")
                ],
                handler: { params in
                    return ["success": true, "trackId": UUID().uuidString]
                }
            )
        ]

        // Register tools with session
        session?.tools = tools
        #endif
    }

    // MARK: - Helper Methods

    private func describeSpectrum(_ spectrum: [Float]) -> String {
        guard !spectrum.isEmpty else { return "No data" }

        let avgLow = spectrum.prefix(spectrum.count / 4).reduce(0, +) / Float(spectrum.count / 4)
        let avgMid = spectrum.dropFirst(spectrum.count / 4).prefix(spectrum.count / 2).reduce(0, +) / Float(spectrum.count / 2)
        let avgHigh = spectrum.suffix(spectrum.count / 4).reduce(0, +) / Float(spectrum.count / 4)

        return "Low: \(String(format: "%.2f", avgLow)), Mid: \(String(format: "%.2f", avgMid)), High: \(String(format: "%.2f", avgHigh))"
    }

    private func calculateOptimalTempo(heartRate: Double, coherence: Float) -> Int {
        // Bio-reactive tempo calculation
        // High coherence = closer to heart rate, low coherence = slower to calm
        let baseTemp = Int(heartRate)
        let adjustment = Int((1 - coherence) * 20)
        return max(60, min(180, baseTemp - adjustment))
    }

    // MARK: - Errors

    enum FoundationModelsError: Error, LocalizedError {
        case sessionNotInitialized
        case modelNotAvailable
        case generationFailed(String)

        var errorDescription: String? {
            switch self {
            case .sessionNotInitialized:
                return "AI session not initialized"
            case .modelNotAvailable:
                return "On-device AI model not available"
            case .generationFailed(let reason):
                return "Generation failed: \(reason)"
            }
        }
    }
}

// MARK: - Generable Macro Compatibility

#if !canImport(FoundationModels)
/// Compatibility wrapper for @Generable macro
@propertyWrapper
struct Generable<T: Codable> {
    var wrappedValue: T

    init(wrappedValue: T) {
        self.wrappedValue = wrappedValue
    }
}

/// Compatibility wrapper for @Guide macro
@propertyWrapper
struct Guide<T> {
    var wrappedValue: T
    let description: String

    init(wrappedValue: T, description: String = "") {
        self.wrappedValue = wrappedValue
        self.description = description
    }
}

/// Tool definition for compatibility
struct Tool {
    let name: String
    let description: String
    var parameters: [ToolParameter] = []
    let handler: ([String: Any]) -> [String: Any]

    enum ToolParameter {
        case string(name: String, description: String)
        case int(name: String, description: String)
        case bool(name: String, description: String)
    }
}
#endif

// MARK: - SwiftUI Integration

import SwiftUI

struct FoundationModelsView: View {
    @State private var engine = FoundationModelsEngine()
    @State private var prompt = ""
    @State private var result = ""
    @State private var isGenerating = false

    var body: some View {
        VStack(spacing: 20) {
            // Status
            HStack {
                Circle()
                    .fill(engine.isReady ? Color.green : Color.red)
                    .frame(width: 12, height: 12)

                Text(engine.processingStatus.rawValue)
                    .font(.caption)
                    .foregroundStyle(.secondary)
            }
            .padding()
            .liquidGlass(.regular, cornerRadius: 12)

            // Input
            LiquidGlassTextField(
                text: $prompt,
                placeholder: "Describe your music idea...",
                icon: "sparkles"
            )

            // Generate button
            Button {
                Task {
                    await generate()
                }
            } label: {
                HStack {
                    if isGenerating {
                        ProgressView()
                            .progressViewStyle(.circular)
                            .tint(.white)
                    } else {
                        Image(systemName: "brain")
                    }
                    Text(isGenerating ? "Generating..." : "Generate with AI")
                }
            }
            .buttonStyle(.liquidGlass(tint: .purple))
            .disabled(!engine.isReady || isGenerating)

            // Result
            if !result.isEmpty {
                ScrollView {
                    Text(result)
                        .font(.body)
                        .foregroundStyle(.white)
                        .frame(maxWidth: .infinity, alignment: .leading)
                        .padding()
                }
                .liquidGlass(.clear, cornerRadius: 16)
            }

            Spacer()
        }
        .padding()
    }

    private func generate() async {
        isGenerating = true
        defer { isGenerating = false }

        do {
            let suggestion = try await engine.generateComposition(
                mood: "energetic",
                genre: prompt.isEmpty ? "electronic" : prompt,
                duration: 180
            )

            result = """
            ðŸŽµ Composition Suggestion

            Tempo: \(suggestion.tempo) BPM
            Key: \(suggestion.key)
            Time: \(suggestion.timeSignature)
            Genre: \(suggestion.genre)
            Mood: \(suggestion.mood)

            Chord Progression:
            \(suggestion.chordProgression.joined(separator: " â†’ "))

            \(suggestion.description)
            """
        } catch {
            result = "Error: \(error.localizedDescription)"
        }
    }
}

#Preview("Foundation Models") {
    ZStack {
        AnimatedGlassBackground()
        FoundationModelsView()
    }
    .preferredColorScheme(.dark)
}
